---
article_type: article
author: "Maarja Olesk"
lang: en
news_title: "Language Technology Professor: Estonian Machine Translation Would Be Impossible Without Open Data"
preview: "The University of Tartu’s machine translation engine has been trained entirely on open data because no organization alone could collect millions of translation samples from all over the world, head of the university’s Chair for Natural Language Processing Mark Fishel stresses in his interview to the Estonian open data portal. Read on to learn what the university’s machine translation engine and speech synthesizer are capable of thanks to data and discover what language technologies have in store for our future."
preview_img: language-tech2.jpg
submitted: 2020/08/31
tags:
  - open data
  - language technology
  - machine translation
  - speech synthesis
  - machine learning
text: |
  <p><b>The University of Tartu’s machine translation engine has been trained entirely on open data because no organization alone could collect millions of translation samples from all over the world, head of the university’s Chair for Natural Language Processing Mark Fishel stresses in his interview to the Estonian open data portal. Read on to learn what the university’s machine translation engine and speech synthesizer are capable of thanks to data and discover what language technologies have in store for our future.</b></p>

  <p><b>Could you first tell us what sort of language technologies your research team at the University of Tartu is working on?</b></p> 
  <p>The research fields of the Chair of Language Technology cover almost everything that has to do with the processing of written text and language analysis. This includes machine translation, speech synthesis, also analysis of Estonian words and phrases and many other topics.</p>
  <p><b>Your team has developed a machine translation program <a href="https://neurotolge.ee/">neurotõlge.ee</a>, which is free for anyone to use. What can the program do? What data and technologies have been used to build it?</b></p>
  <p>The aim of the machine translation program is to provide automatic translation between different languages: input in one language and output in another. But it’s more complicated than this. For example, translation depends on the domain. Colloquial spoken language is translated in one way and official speeches or technical manuals in another. Some words can have several meanings, for example <i>driver</i> in English can be a computer driver or a car driver. In the case of Neurotolge.ee we have tried to take this domain-specific context into account. The user can choose whether they want a text to be translated into formal or more colloquial language.</p> 
  <p>The online translation engine is just a demo but behind this lies an application programming interface that professional translators can integrate with their translation tools. This allows documents to be translated automatically and then reviewed by a human translator. We also develop domain-specific translation modules for translation agencies.</p>
  <p>What makes our machine translation special is that it is multi-lingual and can translate between seven languages: Estonian, Latvian, Lithuanian, Russian, English, German and Finnish. We use the same trained model in all directions. This enables the user to mix different languages in the same text – for example Estonian, German and English – and translate this all into Latvian. The engine can cope with it. We don’t specify the input language, only the output – the user only selects what language they want to translate into.</p>
  <p>An unexpected but useful side effect is translation into the same language, for example translation from Estonian with grammar mistakes to correct Estonian. We haven’t taught the model to do this, it is a longer story why it works like this – but it works.</p>
  <p>Regarding data, Neurotolge.ee is a great example because it is based on machine translation. As developers, we don’t speak Finnish or Lithuanian ourselves, we just need data for training the model. Absolutely all data that we have used for training the program are open data that have been collected by research teams from all over the world and made accessible to everyone. These include transcripts of European Parliament speeches, European Union legislation, or the website OpenSubtitles where people can share subtitles. Based on this, a huge parallel corpus has been created with over 15 million translation segments between Estonian and English alone. It would be impossible for any single organization to independently collect the volume of data required for training machine translation systems. Without open data this would be just impossible. For speech synthesis we have collected some data on our own but for developing translation technologies the usability of the existing open data has been very good.</p>
  <p><b>What other projects besides machine translation are you currently working on?</b></p>
  <p>Another good example is speech synthesis, or automatic production of speech from written text, of which we have a public demo on the website <a href="https://neurokone.ee/">neurokone.ee</a>. This is a pure data science project where we collected data, found the necessary Estonian language texts and people to read these texts out aloud. Based on that we created a dataset with 66 hours of oral speech. We also cooperate with the Institute of the Estonian Language, who has also collected such data – together we have 6 speakers and more than 100 hours of speech. The machine learns to imitate human speech as we feed him both the text and how it should be pronounced. The public demo is not yet perfect, but it already sounds quite natural and performs well.</p>
  <p><b>Is it really possible to develop a speech synthesizer that sounds almost natural based only on 100 hours of human speech?</b></p>
  <p>With speech synthesis, the size of the data doesn’t matter. If we have one speaker, even just 20 hours of material will already make the synthesizer sound close to natural. However, it might struggle with rare words or word combinations if it hasn’t seen them before. In that sense more data can certainly help.</p>
  <p>Compared to translation data, getting access to speech data is more complicated. Some data exists in the Estonian Language Resource Center’s repositories but as soon as our collaboration with the Institute of the Estonian Language ends, we will also publish our datasets in the center’s repository. It should definitely also be linked to the Estonian national open data portal. We also recently published the base components of our machine translation program in the Estonian public AI source code repository and will do the same with the speech synthesis model, so that anyone could reuse them for free.</p>
  <p><b>So the source code of your software is also open?</b></p>
  <p>Yes, the code that enables the reuse of these models is open source. We will also try to share the code that we use for training similar models and preparing the data, but our first priority is to share the code that allows them to be reused. Without this the trained model is of no use.</p>
  <p><b>Do you have an idea who the main users of your applications are, and how many they are?</b></p>
  <p>The public AI application is a relatively new thing, we developed this only in spring. It has attracted a lot of interest but has no definite users yet. But the main users of our machine translation system are translation agencies. There is also a certain amount of people who use the translation application online – maybe they find it useful that our online demo enables comparing our translations with the machine translation of Google and the Tilde translation agency. This helps us compare the quality of translations to see where we – or they – may have problems. At the same time, being able to compare three alternative translations of the same text is very useful for people who don’t speak the language well or don’t trust the output.</p>
  <p><b>You mentioned that the bulk of the data you use are available as open data. Would your work be possible at all if access to the data was restricted somehow or behind a paywall?</b></p>
  <p>For sure we wouldn’t be here today without open data. In some domains such as speech synthesis, it is possible for a research group to compile 100 hours of data on their own. But in the case of translation data you need millions and tens of millions of translation examples that no Estonian or international company could collect alone. So, for us as well as others, open access to data has been vital.</p>
  <p><b>To what extent have you come across legal issues in your work, for example regarding conditions for reusing or redistributing the data that you use?</b></p>
  <p>There are usually no problems with textual data. If the text has been published online, its usage is normally not restricted. If we talk about books or copyrighted material it gets more complicated. Still, for large datasets, researchers are usually able to find sources where there are no issues, or they agree on the conditions for reuse with the owner. Problems emerge with speech data because someone’s voice is part of their personal data and under GDPR (<i>General Data Protection Regulation</i>), a person has the right to say that they no longer agree to their personal data being reused. But even in this case problems would mainly arise if we wanted to share the data or create new datasets based on the data – in this case you must be sure you’re allowed to do this. If we only use the data to develop our models, we are not sharing the data as such.</p>
  <p><b>How well developed are similar technologies in other countries? Is Estonia a leader here or do we have role models that we learn from?</b></p>
  <p>We usually learn from others when we develop practical applications. Of course every research group has their own original ideas and inventions, but in speech synthesis, machine translation or text analysis I’m afraid we cannot claim to be ahead of others. Regarding machine translation we’re involved in a European project with Mozilla and European universities to develop a translation plugin for their web browser. We have also helped organize an international machine translation conference, so we’re also working towards taking a bit of a leading role in the field.</p>
  <p><b>What are your next big goals and where is the world of language technology moving altogether? Dreaming big, what do you think could be possible in 5 or 10 years thanks to the work that you do today?</b></p>
  <p>Dreaming realistically, there are a lot of useful applications in other languages that are absent in Estonian or do not work as well, for example grammar correction. In five or ten years these applications could also become available in Estonian.</p>
  <p>Dreaming big but unealistically, it would be extremely interesting to research what it really means that a person speaks or understands a language, and then apply this understanding to computers so that computers could learn more efficiently. No human being is exposed to such volumes of textual, language or speech data in their lives that are needed for training a computer program – as a learning machine, a human being is so much more efficient! This knowledge could help us take a step towards Artificial General Intelligence.</p>
  <p>Dreaming big but realistically, chatbots or virtual assistants are an interesting field. Very few of them have been developed for the Estonian language so far. Even English-language assistants such as Siri have a very limited understanding. These could be made much more flexible. I don’t believe they will ever completely replace a mouse or a keyboard, but for someone driving or using their phone, it would be much more convenient if they didn’t have to use a specific syntax to communicate with such applications. These technologies could be developed further overall but in particular for the Estonian language. We should be able to communicate with our smart devices in our natural language.</p>

  ______
  <p><em>This interview is part of our blog’s two-part series on the role of data in language technology.</p>
  <p>The Open Data Portal's content is created as part of the EU structural funds' programme 'Raising Public Awareness about the Information Society' financed through the EU Regional Development Fund. The project is implemented by Open Knowledge Estonia.</em></p>'
---
